{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_78K-b5gVeX"
      },
      "outputs": [],
      "source": [
        "##########EXTRACTING INFORMATION FROM CATALOGUE TEXT INTO SPREADSHEET#########\n",
        "\n",
        "# @title\n",
        "# --- Museum Catalogue Transcription with Gemini + Multi-entry Saving Fix ---\n",
        "# Works in Google Colab\n",
        "\n",
        "# --- Step 1: Install necessary libraries ---\n",
        "!pip install pandas ipywidgets openpyxl requests google-genai --quiet\n",
        "\n",
        "# --- Step 2: Imports and setup ---\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML, Image as ColabImage\n",
        "import ipywidgets as widgets\n",
        "import requests\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import files\n",
        "\n",
        "# --- Step 3: Set Gemini API key securely ---\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"INSERTKEYHERE\"  # <-- Replace with your actual key\n",
        "\n",
        "# Initialize Gemini client\n",
        "try:\n",
        "    client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "    print(\"Gemini Client Initialized.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Gemini client: {e}\")\n",
        "    client = None\n",
        "\n",
        "# ---------------------------------------\n",
        "# Define input/output paths\n",
        "# ---------------------------------------\n",
        "drive_folder = \"/content/drive/MyDrive/folder\"  # <-- change path\n",
        "output_excel = os.path.join(drive_folder, \"output.xlsx\") # <-- change output name\n",
        "\n",
        "# Load or initialize DataFrame\n",
        "if os.path.exists(output_excel):\n",
        "    df = pd.read_excel(output_excel)\n",
        "    print(f\"Loaded existing progress file with {len(df)} rows.\")\n",
        "else:\n",
        "    image_files = [f for f in os.listdir(drive_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.tif'))]\n",
        "    df = pd.DataFrame(columns=[\n",
        "        \"Filename\", \"Catalogue number\", \"Object name\", \"Maker/Donor\",\n",
        "        \"Date of creation\", \"Object entry information\", \"Object/Inventory number\",\n",
        "        \"Raw_Response\"\n",
        "    ])\n",
        "    for file in image_files:\n",
        "        df.loc[len(df)] = [file, \"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n",
        "    print(f\"ound {len(image_files)} image(s) in Google Drive folder.\")\n",
        "\n",
        "# Resume progress\n",
        "analyzed_rows = df[\"Raw_Response\"].notna() & (df[\"Raw_Response\"].astype(str).str.strip() != \"\")\n",
        "if analyzed_rows.any():\n",
        "    last_done = df[analyzed_rows].index.max() + 1\n",
        "    print(f\"Resuming from image #{last_done + 1} of {len(df)}...\")\n",
        "else:\n",
        "    last_done = 0\n",
        "    print(\"ðŸš€ Starting fresh analysis...\")\n",
        "\n",
        "columns = df.columns.tolist()\n",
        "\n",
        "# ---------------------------------------\n",
        "# Build widgets\n",
        "# ---------------------------------------\n",
        "search_box = widgets.Text(description='Search:', placeholder='Enter keyword(s)...',\n",
        "                          layout=widgets.Layout(width='60%'), style={'description_width': 'initial'})\n",
        "column_dropdown = widgets.Dropdown(options=['All columns'] + columns, value='All columns',\n",
        "                                   description='Search in:', style={'description_width': 'initial'})\n",
        "search_button = widgets.Button(description=' Search', button_style='success')\n",
        "clear_button = widgets.Button(description='Clear', button_style='warning')\n",
        "prev_button = widgets.Button(description='Prev')\n",
        "next_button = widgets.Button(description='Next')\n",
        "analyze_button = widgets.Button(description='Analyze Image', button_style='primary')\n",
        "save_button = widgets.Button(description='Save Now', button_style='info')\n",
        "output = widgets.Output()\n",
        "\n",
        "current_index = last_done\n",
        "filtered_df = df.copy()\n",
        "\n",
        "# ---------------------------------------\n",
        "# Helper functions\n",
        "# ---------------------------------------\n",
        "def get_current_filename():\n",
        "    if filtered_df.empty:\n",
        "        return None\n",
        "    record = filtered_df.iloc[current_index]\n",
        "    return record[\"Filename\"]\n",
        "\n",
        "def display_record(idx):\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        if filtered_df.empty:\n",
        "            display(HTML(\"<h4>No matching results.</h4>\"))\n",
        "            analyze_button.disabled = True\n",
        "            return\n",
        "        record = filtered_df.iloc[idx]\n",
        "        info_html = f\"<h3>ðŸ“– Record ({idx + 1} / {len(filtered_df)})</h3><table>\"\n",
        "        for col in columns:\n",
        "            val = str(record[col])\n",
        "            if len(val) > 400:\n",
        "                val = val[:400] + \"...\"\n",
        "            info_html += f\"<tr><td><b>{col}</b></td><td>{val}</td></tr>\"\n",
        "        info_html += \"</table><br>\"\n",
        "        display(HTML(info_html))\n",
        "        img_path = os.path.join(drive_folder, record[\"Filename\"])\n",
        "        if os.path.exists(img_path):\n",
        "            display(ColabImage(filename=img_path, width=400))\n",
        "        else:\n",
        "            display(HTML(\"<p><i>Image file not found.</i></p>\"))\n",
        "            analyze_button.disabled = True\n",
        "\n",
        "def save_progress():\n",
        "    df.to_excel(output_excel, index=False)\n",
        "    print(f\"Progress saved to {output_excel}\")\n",
        "\n",
        "def parse_multiple_entries(text):\n",
        "    \"\"\"Parse multiple entries per page with full multiline field capture.\"\"\"\n",
        "\n",
        "    # Define the exact catalogue field labels\n",
        "    field_labels = [\n",
        "        \"Continuation from previous page\",\n",
        "        \"Catalogue number\",\n",
        "        \"Object name\",\n",
        "        \"Maker/Donor\",\n",
        "        \"Date of creation\",\n",
        "        \"Object entry information\",\n",
        "        \"Object/Inventory number\"\n",
        "    ]\n",
        "\n",
        "    # Build a regex-safe alternation of labels\n",
        "    label_pattern = \"|\".join([re.escape(lbl) for lbl in field_labels])\n",
        "\n",
        "    # Split into entries only on Catalogue number OR Continuation\n",
        "    blocks = re.split(r\"(?=Continuation from previous page\\s*:|Catalogue number\\s*:)\",\n",
        "                      text, flags=re.IGNORECASE)\n",
        "\n",
        "    entries = []\n",
        "\n",
        "    for block in blocks:\n",
        "        block = block.strip()\n",
        "        if not block:\n",
        "            continue\n",
        "\n",
        "        entry = {lbl: \"N/A\" for lbl in field_labels}\n",
        "        entry[\"Continuation\"] = \"N/A\"\n",
        "\n",
        "        for lbl in field_labels:\n",
        "            # Regex:\n",
        "            # <label> : ( capture everything ) until the next LABEL or end of string\n",
        "            pattern = rf\"{re.escape(lbl)}\\s*:\\s*(.*?)(?=\\n(?:{label_pattern})\\s*:|$)\"\n",
        "            match = re.search(pattern, block, flags=re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "            if match:\n",
        "                val = match.group(1).strip()\n",
        "                if lbl == \"Continuation from previous page\":\n",
        "                    entry[\"Continuation\"] = val\n",
        "                else:\n",
        "                    entry[lbl] = val\n",
        "\n",
        "        entries.append(entry)\n",
        "\n",
        "    return entries\n",
        "\n",
        "# ---------------------------------------\n",
        "# Main analysis\n",
        "# ---------------------------------------\n",
        "def perform_visual_analysis(_=None):\n",
        "    \"\"\"Perform OCR + structured transcription with multiple entries per page (safe global append).\"\"\"\n",
        "    global df, filtered_df, current_index\n",
        "\n",
        "    if client is None:\n",
        "        with output:\n",
        "            display(HTML(\"<h4>Gemini client not initialized. Check API key.</h4>\"))\n",
        "        return\n",
        "\n",
        "    filename = get_current_filename()\n",
        "    if not filename:\n",
        "        with output:\n",
        "            display(HTML(\"<h4>No image found for this record.</h4>\"))\n",
        "        return\n",
        "\n",
        "    analyze_button.disabled = True\n",
        "    analyze_button.description = \"Analyzing...\"\n",
        "    with output:\n",
        "        display(HTML(f\"<h4>Analyzing image: <b>{filename}</b></h4>\"))\n",
        "\n",
        "    img_path = os.path.join(drive_folder, filename)\n",
        "    try:\n",
        "        with open(img_path, \"rb\") as f:\n",
        "            img_bytes = f.read()\n",
        "        image = types.Part.from_bytes(data=img_bytes, mime_type=\"image/jpeg\")\n",
        "\n",
        "        prompt = \"\"\"\n",
        "        This is a scanned museum catalogue page. Transcribe all object entries found on this page.\n",
        "\n",
        "        Each page may contain several catalogue entries. For every entry, return:\n",
        "\n",
        "        Continuation from previous page: [TRUE/FALSE]\n",
        "        Catalogue number:\n",
        "        Object name:\n",
        "        Maker/Donor:\n",
        "        Date of creation:\n",
        "        Object entry information:\n",
        "        Object/Inventory number:\n",
        "\n",
        "        Detect whether the top text is a continuation of a previous entry\n",
        "        (no catalogue number or object name at the top = continuation TRUE).\n",
        "\n",
        "        If a value cannot be identified, use \"N/A\".\n",
        "        Keep this exact field order and repeat the block for each entry.\n",
        "        \"\"\"\n",
        "\n",
        "        # --- Retry logic ---\n",
        "        result_text = \"\"\n",
        "        for attempt in range(1, 3):\n",
        "            try:\n",
        "                response = client.models.generate_content(\n",
        "                    model=\"gemini-2.5-flash-preview-09-2025\", ###Change model here if needed\n",
        "                    contents=[prompt, image]\n",
        "                )\n",
        "                if hasattr(response, \"text\") and response.text:\n",
        "                    result_text = response.text.strip()\n",
        "                elif hasattr(response, \"candidates\") and response.candidates:\n",
        "                    try:\n",
        "                        result_text = response.candidates[0].content.parts[0].text.strip()\n",
        "                    except Exception:\n",
        "                        result_text = \"\"\n",
        "                if result_text:\n",
        "                    break\n",
        "                print(f\"Empty response on attempt {attempt}, retrying...\")\n",
        "                time.sleep(3)\n",
        "            except Exception as inner_e:\n",
        "                print(f\"Retry {attempt} failed: {inner_e}\")\n",
        "                time.sleep(3)\n",
        "\n",
        "        if not result_text:\n",
        "            raise ValueError(\"Model returned an empty or invalid response after retries.\")\n",
        "\n",
        "        # Parse all entries\n",
        "        entries = parse_multiple_entries(result_text)\n",
        "        if not entries:\n",
        "            raise ValueError(\"No entries parsed from Gemini output.\")\n",
        "\n",
        "        # SAFELY remove old rows and append new ones\n",
        "        df.drop(df[df[\"Filename\"] == filename].index, inplace=True)\n",
        "        new_df = pd.DataFrame([\n",
        "            {\n",
        "                \"Filename\": filename,\n",
        "                \"Catalogue number\": e.get(\"Catalogue number\", \"N/A\"),\n",
        "                \"Object name\": e.get(\"Object name\", \"N/A\"),\n",
        "                \"Maker/Donor\": e.get(\"Maker/Donor\", \"N/A\"),\n",
        "                \"Date of creation\": e.get(\"Date of creation\", \"N/A\"),\n",
        "                \"Object entry information\": e.get(\"Object entry information\", \"N/A\"),\n",
        "                \"Object/Inventory number\": e.get(\"Object/Inventory number\", \"N/A\"),\n",
        "                \"Raw_Response\": result_text\n",
        "            }\n",
        "            for e in entries\n",
        "        ])\n",
        "\n",
        "        # Append to the global dataframe in place\n",
        "        df = pd.concat([df, new_df], ignore_index=True)\n",
        "\n",
        "        # Save progress and refresh filtered view\n",
        "        save_progress()\n",
        "        filtered_df = df.copy()\n",
        "\n",
        "        # --- Display summary ---\n",
        "        with output:\n",
        "            display(HTML(f\"<h4>Analysis Complete for <b>{filename}</b> â€” {len(entries)} entries found.</h4>\"))\n",
        "            for i, e in enumerate(entries, 1):\n",
        "                block = \"<pre style='background:#f6f8fa; padding:10px; border-radius:6px;'>\"\n",
        "                block += f\"<b>Entry {i}</b>\\n\" + \"\\n\".join([f\"{k}: {v}\" for k, v in e.items()])\n",
        "                block += \"</pre>\"\n",
        "                display(HTML(block))\n",
        "            display(HTML(\"<h5>Full Raw Response:</h5>\"))\n",
        "            display(HTML(f\"<pre style='white-space:pre-wrap; background:#eef; padding:10px; border-radius:6px;'>{result_text}</pre>\"))\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        with output:\n",
        "            display(HTML(f\"<h4>Error while analyzing <b>{filename}</b>: {e}</h4>\"))\n",
        "\n",
        "    finally:\n",
        "        analyze_button.description = \"Analyze Image\"\n",
        "        analyze_button.disabled = False\n",
        "\n",
        "# ---------------------------------------\n",
        "# Widget bindings\n",
        "# ---------------------------------------\n",
        "def perform_search(_=None):\n",
        "    global filtered_df, current_index\n",
        "    query = search_box.value.strip().lower()\n",
        "    selected = column_dropdown.value\n",
        "    if not query:\n",
        "        filtered_df = df.copy()\n",
        "    else:\n",
        "        if selected == \"All columns\":\n",
        "            mask = df.apply(lambda r: r.astype(str).str.lower().str.contains(query).any(), axis=1)\n",
        "        else:\n",
        "            mask = df[selected].astype(str).str.lower().str.contains(query)\n",
        "        filtered_df = df[mask]\n",
        "    current_index = 0\n",
        "    display_record(current_index)\n",
        "\n",
        "def clear_search(_):\n",
        "    global filtered_df, current_index\n",
        "    search_box.value = \"\"\n",
        "    column_dropdown.value = \"All columns\"\n",
        "    filtered_df = df.copy()\n",
        "    current_index = 0\n",
        "    display_record(current_index)\n",
        "\n",
        "def on_next(_):\n",
        "    global current_index\n",
        "    if not filtered_df.empty:\n",
        "        current_index = (current_index + 1) % len(filtered_df)\n",
        "        display_record(current_index)\n",
        "\n",
        "def on_prev(_):\n",
        "    global current_index\n",
        "    if not filtered_df.empty:\n",
        "        current_index = (current_index - 1) % len(filtered_df)\n",
        "        display_record(current_index)\n",
        "\n",
        "search_button.on_click(perform_search)\n",
        "search_box.on_submit(perform_search)\n",
        "clear_button.on_click(clear_search)\n",
        "next_button.on_click(on_next)\n",
        "prev_button.on_click(on_prev)\n",
        "analyze_button.on_click(perform_visual_analysis)\n",
        "save_button.on_click(lambda _: save_progress())\n",
        "\n",
        "# ---------------------------------------\n",
        "# Display UI\n",
        "# ---------------------------------------\n",
        "controls = widgets.HBox([search_box, column_dropdown, search_button, clear_button])\n",
        "nav = widgets.HBox([prev_button, next_button, analyze_button, save_button])\n",
        "ui = widgets.VBox([controls, nav, output])\n",
        "display(ui)\n",
        "\n",
        "with output:\n",
        "    output.clear_output()\n",
        "    display_record(current_index)\n",
        "\n",
        "#display_record(current_index)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Setup and Imports ---\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Step 2: Mount Google Drive ---\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"Drive mounted.\\n\")\n",
        "\n",
        "# --- Step 3: Define file paths ---\n",
        "excel_path = '/content/drive/MyDrive/input.xlsx' # <-- change path\n",
        "csv_url = \"https://coimages.sciencemuseumgroup.org.uk/datasets/smg_object_records_all_09_04_2025.csv\" ### <-- KEEP URL for retrieving object records\n",
        "output_path = '/content/drive/MyDrive/output.xlsx' # <-- change path\n",
        "\n",
        "# --- Step 4: Load Excel file ---\n",
        "print(\"Loading Excel file...\")\n",
        "start_time = time.time()\n",
        "df_excel = pd.read_excel(excel_path)\n",
        "print(f\"Loaded Excel file with {len(df_excel)} rows and {len(df_excel.columns)} columns.\")\n",
        "print(f\"Columns found: {list(df_excel.columns)}\\n\")\n",
        "\n",
        "# --- Step 5: Extract and clean Object/Inventory numbers ---\n",
        "print(\"Extracting 'Object/Inventory number' entries matching pattern like 1915â€“155 (anywhere in text)...\")\n",
        "\n",
        "col_name = \"Object/Inventory number\"\n",
        "if col_name not in df_excel.columns:\n",
        "    raise ValueError(f\"Column '{col_name}' not found in the Excel file. Please verify the column name.\")\n",
        "\n",
        "df_excel[col_name] = df_excel[col_name].astype(str).str.replace(\"â€“\", \"-\")\n",
        "\n",
        "pattern = re.compile(r\"(\\d{1,6}\\s*[-â€“]\\s*\\d{1,6})\")\n",
        "\n",
        "# Always keep all rows\n",
        "df_excel[\"extracted_ids\"] = df_excel[col_name].apply(lambda x: pattern.findall(x))\n",
        "\n",
        "# Normalize IDs for matching\n",
        "df_excel[\"clean_id\"] = df_excel[\"extracted_ids\"].apply(\n",
        "    lambda lst: [re.sub(r\"\\s+\", \"\", i) for i in lst] if lst else []\n",
        ")\n",
        "\n",
        "# Build ID universe (only from rows that actually have IDs)\n",
        "all_ids = [i for sublist in df_excel[\"clean_id\"] for i in sublist]\n",
        "unique_ids = sorted(set(all_ids))\n",
        "id_set = set(unique_ids)\n",
        "\n",
        "print(f\"Found {len(unique_ids)} unique valid inventory-like IDs across {len(df_excel)} rows.\\n\")\n",
        "print(f\"Unique formatted IDs ready for matching: {len(id_set)}\\n\")\n",
        "\n",
        "# --- Step 6: Prepare for chunked reading of large CSV ---\n",
        "print(\"Starting chunked read of large CSV file...\")\n",
        "chunk_size = 15000\n",
        "matched_chunks = []\n",
        "total_rows = 0\n",
        "match_count = 0\n",
        "start_csv_time = time.time()\n",
        "\n",
        "first_chunk = pd.read_csv(csv_url, nrows=5)\n",
        "cols = list(first_chunk.columns)\n",
        "print(f\"CSV columns: {cols}\\n\")\n",
        "\n",
        "# --- Step 7: Read CSV in chunks and search for matches ---\n",
        "print(\"Searching for matches in the 'identifier' column...\")\n",
        "for i, chunk in enumerate(\n",
        "    pd.read_csv(csv_url, chunksize=chunk_size, usecols=['identifier'], low_memory=False)\n",
        "):\n",
        "    total_rows += len(chunk)\n",
        "\n",
        "    chunk[\"identifier\"] = (\n",
        "        chunk[\"identifier\"]\n",
        "        .astype(str)\n",
        "        .str.replace(\"â€“\", \"-\")\n",
        "        .str.replace(\" \", \"\")\n",
        "    )\n",
        "\n",
        "    matches = chunk[chunk[\"identifier\"].isin(id_set)]\n",
        "\n",
        "    if not matches.empty:\n",
        "        print(f\"Found {len(matches)} matches in chunk {i+1}.\")\n",
        "        match_count += len(matches)\n",
        "        matched_chunks.append(matches)\n",
        "\n",
        "    elapsed = time.time() - start_csv_time\n",
        "    est_total_chunks = 500000 / chunk_size\n",
        "    est_remaining = max((elapsed / (i + 1)) * (est_total_chunks - (i + 1)), 0)\n",
        "    print(\n",
        "        f\"Processed {total_rows:,} rows. \"\n",
        "        f\"Estimated time left: ~{est_remaining/60:.1f} minutes\\n\"\n",
        "    )\n",
        "\n",
        "# --- Step 8: Combine matches and re-read full info ---\n",
        "if matched_chunks:\n",
        "    all_matches = pd.concat(matched_chunks)\n",
        "    unique_ids_matched = all_matches[\"identifier\"].unique()\n",
        "    print(f\"Total unique matching IDs found: {len(unique_ids_matched)}\")\n",
        "else:\n",
        "    print(\"No matches found.\")\n",
        "    unique_ids_matched = []\n",
        "\n",
        "# --- Step 9: Load only matching rows fully from CSV ---\n",
        "if len(unique_ids_matched) > 0:\n",
        "    print(\"Re-reading full rows for matched identifiers...\")\n",
        "    matched_data = []\n",
        "    for chunk in pd.read_csv(csv_url, chunksize=15000, low_memory=False):\n",
        "        chunk[\"identifier\"] = (\n",
        "            chunk[\"identifier\"]\n",
        "            .astype(str)\n",
        "            .str.replace(\"â€“\", \"-\")\n",
        "            .str.replace(\" \", \"\")\n",
        "        )\n",
        "        matched_rows = chunk[chunk[\"identifier\"].isin(unique_ids_matched)]\n",
        "        if not matched_rows.empty:\n",
        "            matched_data.append(matched_rows)\n",
        "\n",
        "    df_csv_matched = pd.concat(matched_data)\n",
        "    print(f\"Loaded {len(df_csv_matched)} matched rows from CSV.\\n\")\n",
        "else:\n",
        "    df_csv_matched = pd.DataFrame()\n",
        "\n",
        "# --- Step 10: Merge (LEFT JOIN, preserving ID-less rows) ---\n",
        "print(\"Merging matched data from both sources (preserving all Excel rows)...\")\n",
        "\n",
        "# Explode IDs; rows with no IDs produce NaN clean_id and are preserved\n",
        "exploded_excel = df_excel.explode(\"clean_id\")\n",
        "\n",
        "merged = pd.merge(\n",
        "    exploded_excel,\n",
        "    df_csv_matched,\n",
        "    left_on=\"clean_id\",\n",
        "    right_on=\"identifier\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "print(f\"Merge complete. Combined rows (including ID-less rows): {len(merged)}\\n\")\n",
        "\n",
        "# --- Step 11: Save results ---\n",
        "print(\"Saving results to Excel & CSV...\")\n",
        "\n",
        "merged.to_excel(output_path, index=False)\n",
        "\n",
        "csv_output_path = output_path.replace(\".xlsx\", \".csv\")\n",
        "merged.to_csv(csv_output_path, index=False)\n",
        "\n",
        "print(\n",
        "    f\"Done! Files saved:\\n\"\n",
        "    f\"- Excel: {output_path}\\n\"\n",
        "    f\"- CSV:   {csv_output_path}\"\n",
        ")"
      ],
      "metadata": {
        "id": "GzlcF075swJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee1T5reokw1N"
      },
      "outputs": [],
      "source": [
        "# --- Dataset Comparison: Old Catalogue vs Science Museum Group ---\n",
        "# Retains verbose feedback, timing, retry logic, and logs for spider chart preparation\n",
        "\n",
        "!pip install pandas openpyxl requests google-genai tqdm --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import os, time, json, random, re\n",
        "from tqdm import tqdm\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import files\n",
        "from datetime import datetime\n",
        "\n",
        "# --- Step 1: Gemini API setup ---\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"INSERTAPIKEYHERE\"  ### INSERT API KEY\n",
        "client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "print(\"Gemini client initialized.\")\n",
        "\n",
        "# --- Step 2: Load dataset ---\n",
        "input_path = \"/content/drive/MyDrive/input.xlsx\"\n",
        "df = pd.read_excel(input_path)\n",
        "print(f\"Loaded {len(df)} rows from {os.path.basename(input_path)}\")\n",
        "\n",
        "# --- Step 3: Define comparison column sets (generic names: old vs new) ---\n",
        "\n",
        "old_cols = [\n",
        "    \"Object name\",\n",
        "    \"Maker/Donor\",\n",
        "    \"Date of creation\",\n",
        "    \"Object entry information\",\n",
        "    \"Object/Inventory number\"\n",
        "]\n",
        "\n",
        "new_cols = [\n",
        "    \"title\",\n",
        "    \"description\",\n",
        "    \"category\",\n",
        "    \"object_name\",\n",
        "    \"date\",\n",
        "    \"place\",\n",
        "    \"maker\"\n",
        "]\n",
        "\n",
        "# --- Step 4: Safe Gemini call with retry and feedback ---\n",
        "\n",
        "def extract_json_from_text(text):\n",
        "    \"\"\"Extract the first valid JSON object from text safely.\"\"\"\n",
        "    text = text.strip()\n",
        "    text = text.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "    match = re.search(r\"\\{[\\s\\S]*\\}\", text)\n",
        "    if match:\n",
        "        snippet = match.group(0)\n",
        "        snippet = re.sub(r\",\\s*([\\]}])\", r\"\\1\", snippet)\n",
        "        snippet = snippet.replace(\"â€¦\", \"\").replace(\"...\", \"\")\n",
        "        return snippet\n",
        "    return None\n",
        "\n",
        "\n",
        "def call_gemini_with_retry(prompt, max_retries=4, base_delay=5):\n",
        "    \"\"\"Call Gemini safely with exponential backoff and improved JSON cleaning.\"\"\"\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            response = client.models.generate_content(\n",
        "                model=\"gemini-2.5-flash-lite\", #### Change Model if needed\n",
        "                contents=prompt\n",
        "            )\n",
        "            raw = response.text.strip()\n",
        "            print(f\"ðŸ§¾ Gemini raw output (first 400 chars): {raw[:400]}...\\n\")\n",
        "\n",
        "            cleaned = extract_json_from_text(raw)\n",
        "            if not cleaned:\n",
        "                raise ValueError(\"No valid JSON found in Gemini output\")\n",
        "\n",
        "            try:\n",
        "                return json.loads(cleaned)\n",
        "            except json.JSONDecodeError:\n",
        "                cleaned = cleaned.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
        "                cleaned = cleaned.replace(\"\\\\\", \"\\\\\\\\\")\n",
        "                cleaned = re.sub(r\"[\\x00-\\x1f\\x7f-\\x9f]\", \"\", cleaned)\n",
        "                return json.loads(cleaned)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Gemini parse error (attempt {attempt}): {e}\")\n",
        "        sleep_time = base_delay * attempt + random.uniform(0, 2)\n",
        "        print(f\"Waiting {sleep_time:.1f}s before retry...\")\n",
        "        time.sleep(sleep_time)\n",
        "    return None\n",
        "\n",
        "# --- Step 5: Build comparison prompt (generic old vs new) ---\n",
        "\n",
        "def make_prompt(old_text, new_text):\n",
        "    return f\"\"\"\n",
        "    You are comparing two museum object catalogue entries referring to the same physical object.\n",
        "    One comes from an older printed catalogue, and the other from the modern Science Museum Group database.\n",
        "\n",
        "    --- OLD Catalogue Entry ---\n",
        "    {old_text}\n",
        "\n",
        "    --- NEW (Science Museum Group) Entry ---\n",
        "    {new_text}\n",
        "\n",
        "    Task:\n",
        "    1. Identify and count entities and details in each entry, categorizing them as:\n",
        "       - Object names\n",
        "       - Makers / Donors\n",
        "       - Organisations/institutions\n",
        "       - Dates or date ranges\n",
        "       - Places\n",
        "       - Descriptive details (form, use, scientific role, etc.)\n",
        "       - Other factual attributes\n",
        "\n",
        "    2. Compute total counts per category for both OLD and NEW datasets.\n",
        "\n",
        "    3. Identify which entities appear uniquely in one entry but not the other.\n",
        "\n",
        "    Return ONLY valid JSON in the following structure:\n",
        "    {{\n",
        "      \"old_counts\": {{\n",
        "          \"object_names\": int,\n",
        "          \"makers\": int,\n",
        "          \"organisations/institutions\": int,\n",
        "          \"dates\": int,\n",
        "          \"places\": int,\n",
        "          \"descriptions\": int,\n",
        "          \"other\": int,\n",
        "          \"total\": int\n",
        "      }},\n",
        "      \"new_counts\": {{\n",
        "          \"object_names\": int,\n",
        "          \"makers\": int,\n",
        "          \"organisations/institutions\": int,\n",
        "          \"dates\": int,\n",
        "          \"places\": int,\n",
        "          \"descriptions\": int,\n",
        "          \"other\": int,\n",
        "          \"total\": int\n",
        "      }},\n",
        "      \"unique_to_old\": {{\n",
        "          \"object_names\": [list],\n",
        "          \"makers\": [list],\n",
        "          \"organisations/institutions\": [list],\n",
        "          \"dates\": [list],\n",
        "          \"places\": [list],\n",
        "          \"descriptions\": [list],\n",
        "          \"other\": [list]\n",
        "      }},\n",
        "      \"unique_to_new\": {{\n",
        "          \"object_names\": [list],\n",
        "          \"makers\": [list],\n",
        "          \"organisations/institutions\": [list],\n",
        "          \"dates\": [list],\n",
        "          \"places\": [list],\n",
        "          \"descriptions\": [list],\n",
        "          \"other\": [list]\n",
        "      }}\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "# --- Step 6: Resume support ---\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/output1.xlsx\"\n",
        "log_path = \"/content/drive/MyDrive/output2.xlsx\"\n",
        "\n",
        "if os.path.exists(output_path):\n",
        "    out_df = pd.read_excel(output_path)\n",
        "    processed = set(out_df[\"RowIndex\"])\n",
        "    print(f\"Resuming from existing results ({len(processed)} processed).\")\n",
        "else:\n",
        "    out_df = pd.DataFrame()\n",
        "    processed = set()\n",
        "\n",
        "logs = []\n",
        "rows_out = []\n",
        "start_time_all = time.time()\n",
        "\n",
        "FAST_MODE = False\n",
        "MAX_ROWS = 10\n",
        "\n",
        "# --- Step 7: Main processing loop ---\n",
        "\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    if FAST_MODE and idx >= MAX_ROWS:\n",
        "        print(\"Fast mode enabled â€” stopping early for testing.\")\n",
        "        break\n",
        "    if idx in processed:\n",
        "        continue\n",
        "\n",
        "    old_text = \" \".join(str(row[c]) for c in old_cols if c in df.columns and pd.notna(row[c]))\n",
        "    new_text = \" \".join(str(row[c]) for c in new_cols if c in df.columns and pd.notna(row[c]))\n",
        "\n",
        "    if not old_text.strip() and not new_text.strip():\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nðŸ”¹ Row {idx + 1}/{len(df)} â€” comparing entry\")\n",
        "    prompt = make_prompt(old_text, new_text)\n",
        "    print(f\"Prompt length: {len(prompt)} characters\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    result = call_gemini_with_retry(prompt)\n",
        "    elapsed = round(time.time() - start_time, 1)\n",
        "    print(f\"Completed in {elapsed}s\")\n",
        "\n",
        "    if not result:\n",
        "        print(f\"Failed for row {idx}, skipping.\\n\")\n",
        "        logs.append({\"RowIndex\": idx, \"Status\": \"Failed\", \"Duration\": elapsed})\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        old, new = result[\"old_counts\"], result[\"new_counts\"]\n",
        "\n",
        "        row_out = {\n",
        "            \"RowIndex\": idx,\n",
        "            \"Old_Total\": old.get(\"total\", 0),\n",
        "            \"New_Total\": new.get(\"total\", 0),\n",
        "            \"Old_ObjectNames\": old.get(\"object_names\", 0),\n",
        "            \"New_ObjectNames\": new.get(\"object_names\", 0),\n",
        "            \"Old_Makers\": old.get(\"makers\", 0),\n",
        "            \"New_Makers\": new.get(\"makers\", 0),\n",
        "            \"Old_Organisations/Institutions\": old.get(\"organisations/institutions\", 0),\n",
        "            \"New_Organisations/Institutions\": new.get(\"organisations/institutions\", 0),\n",
        "            \"Old_Dates\": old.get(\"dates\", 0),\n",
        "            \"New_Dates\": new.get(\"dates\", 0),\n",
        "            \"Old_Places\": old.get(\"places\", 0),\n",
        "            \"New_Places\": new.get(\"places\", 0),\n",
        "            \"Old_Descriptions\": old.get(\"descriptions\", 0),\n",
        "            \"New_Descriptions\": new.get(\"descriptions\", 0),\n",
        "            \"Old_Other\": old.get(\"other\", 0),\n",
        "            \"New_Other\": new.get(\"other\", 0),\n",
        "            \"Unique_to_Old\": json.dumps(result.get(\"unique_to_old\", {})),\n",
        "            \"Unique_to_New\": json.dumps(result.get(\"unique_to_new\", {}))\n",
        "        }\n",
        "\n",
        "        rows_out.append(row_out)\n",
        "        out_df = pd.concat([out_df, pd.DataFrame([row_out])], ignore_index=True)\n",
        "        out_df.to_excel(output_path, index=False)\n",
        "\n",
        "        # Save CSV version\n",
        "        csv_output_path = output_path.replace(\".xlsx\", \".csv\")\n",
        "        out_df.to_csv(csv_output_path, index=False)\n",
        "\n",
        "        logs.append({\n",
        "            \"RowIndex\": idx,\n",
        "            \"Status\": \"Success\",\n",
        "            \"Duration\": elapsed,\n",
        "            \"Timestamp\": datetime.now().isoformat()\n",
        "        })\n",
        "        pd.DataFrame(logs).to_excel(log_path, index=False)\n",
        "\n",
        "        print(f\"Saved row {idx} â€” OLD={old.get('total', 0)}, NEW={new.get('total', 0)}\\n\")\n",
        "        time.sleep(random.uniform(1.0, 2.0))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to parse/save row {idx}: {e}\")\n",
        "        logs.append({\"RowIndex\": idx, \"Status\": f\"Error: {e}\", \"Duration\": elapsed})\n",
        "\n",
        "total_time = round(time.time() - start_time_all, 1)\n",
        "print(f\"\\nCompleted comparison for {len(rows_out)} rows.\")\n",
        "print(f\"Results: {output_path}\")\n",
        "print(f\"Logs: {log_path}\")\n",
        "print(f\"Total time: {total_time / 60:.2f} min\")\n",
        "\n",
        "files.download(output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pw4GaPQAmoSu"
      },
      "outputs": [],
      "source": [
        "########################IMPROVED DOCUMENTATION PROMPT WITH LLM LANGUAGE CHECKS#################\n",
        "############DOCUMENTATION CHECKS############\n",
        "# --- Historic Catalogue Schema Extraction Script ---\n",
        "# Retains XLSX + CSV output, logs, retry logic, etc.\n",
        "# NOW uses LLM to detect harmful and outdated language.\n",
        "\n",
        "!pip install pandas openpyxl requests google-genai tqdm --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import os, time, json, random, re\n",
        "from tqdm import tqdm\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import files\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Clean illegal Excel characters\n",
        "# ---------------------------------------------\n",
        "def clean_excel_string(s):\n",
        "    if isinstance(s, str):\n",
        "        return re.sub(r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]\", \"\", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Step 1: Gemini API setup\n",
        "# ---------------------------------------------\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"INSERTAPIKEYHERE\"  ### Change to your API KEY\n",
        "client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "print(\"Gemini client initialized.\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Step 2: Load dataset\n",
        "# ---------------------------------------------\n",
        "input_path = \"/content/drive/MyDrive/input.xlsx\"\n",
        "df = pd.read_excel(input_path)\n",
        "print(f\"Loaded {len(df)} rows from {os.path.basename(input_path)}\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Step 3: Historic text columns\n",
        "# ---------------------------------------------\n",
        "historic_cols = [\n",
        "    \"Object name\",\n",
        "    \"Maker/Donor\",\n",
        "    \"Date of creation\",\n",
        "    \"Object entry information\",\n",
        "    \"Object/Inventory number\"\n",
        "]\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Step 4: Safe Gemini call with retry\n",
        "# ---------------------------------------------\n",
        "\n",
        "def extract_json_from_text(text):\n",
        "    text = text.strip()\n",
        "    text = text.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "    match = re.search(r\"\\{[\\s\\S]*\\}\", text)\n",
        "    if match:\n",
        "        snippet = match.group(0)\n",
        "        snippet = re.sub(r\",\\s*([\\]}])\", r\"\\1\", snippet)\n",
        "        snippet = snippet.replace(\"â€¦\", \"\").replace(\"...\", \"\")\n",
        "        return snippet\n",
        "    return None\n",
        "\n",
        "\n",
        "def call_gemini_with_retry(prompt, max_retries=4, base_delay=5):\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            response = client.models.generate_content(\n",
        "                model=\"gemini-2.5-flash-lite\",\n",
        "                contents=prompt\n",
        "            )\n",
        "            raw = response.text.strip()\n",
        "            print(f\"ðŸ§¾ Gemini raw output (first 400 chars): {raw[:400]}...\\n\")\n",
        "\n",
        "            cleaned = extract_json_from_text(raw)\n",
        "            if not cleaned:\n",
        "                raise ValueError(\"No valid JSON found in Gemini output\")\n",
        "\n",
        "            try:\n",
        "                return json.loads(cleaned)\n",
        "            except json.JSONDecodeError:\n",
        "                cleaned = cleaned.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
        "                cleaned = cleaned.replace(\"\\\\\", \"\\\\\\\\\")\n",
        "                cleaned = re.sub(r\"[\\x00-\\x1f\\x7f-\\x9f]\", \"\", cleaned)\n",
        "                return json.loads(cleaned)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Gemini parse error (attempt {attempt}): {e}\")\n",
        "\n",
        "        sleep_time = base_delay * attempt + random.uniform(0, 2)\n",
        "        print(f\" Waiting {sleep_time:.1f}s before retry...\")\n",
        "        time.sleep(sleep_time)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Step 5: Prompt including harmful/outdated detection\n",
        "# ---------------------------------------------\n",
        "\n",
        "def make_prompt(entry_text):\n",
        "    return f\"\"\"\n",
        "You are analysing historic museum catalogue entries whose terminology and structure\n",
        "predate modern museum documentation standards. Your task is to perform THREE functions:\n",
        "\n",
        "1) Detect whether each modern museum information type is present within the historic entry.\n",
        "2) Extract the entities or values associated with that information type.\n",
        "3) Detect any harmful or outdated language present in the entry (details below).\n",
        "\n",
        "Historic catalogues often contain information in implicit or differently-structured ways.\n",
        "Interpret the historic phrasing and map it onto the modern extraction schema below.\n",
        "\n",
        "--------------------------------------------------------------------\n",
        "TRANSLATED EXTRACTION SCHEMA (TARGET FORMAT)\n",
        "--------------------------------------------------------------------\n",
        "\n",
        "ESSENTIAL\n",
        "- object_number: catalogue number of the object (primary identifier appearing before name)\n",
        "- secondary_identifier: object ID or other identifying number appearing toward the end of the description\n",
        "- whole_or_part: determine whether the entry describes the whole object (\"Whole\") or a part of an object (\"Part\")\n",
        "- item_count: the number of physically separate items mentioned; if unknown â†’ \"N/A\"\n",
        "- collection: name of the catalogue only\n",
        "- legal_status: copyright, owner, donor, purchase or other legal statement; if none stated â†’ null\n",
        "- object_name: name/title of the object or catalogue entry\n",
        "- description: the descriptive text of the catalogue entry\n",
        "- location: where the item is stored or displayed; if not mentioned â†’ null\n",
        "\n",
        "CORE\n",
        "- hazards: list hazardous materials or warnings referenced; separate with \";\"\n",
        "- makers_and_associated_people:\n",
        "    list of {{ \"name\": <string>, \"role\": <string> }}\n",
        "- place_made: places explicitly stated as where the object was made or used; output as list or null\n",
        "- date_made: when the object was made\n",
        "\n",
        "ENHANCED\n",
        "- materials: extract all materials explicitly stated as making up the object or being a product of the object; separated by \";\"\n",
        "- measurements: all dimensional information; separated by \";\"\n",
        "- interpretation: any contextual, historical, significance-related or interpretive statements\n",
        "- interpretation_source: references, publications, authors, or sources for interpretation\n",
        "- interpretation_date: use catalogue publication date unless another specific interpretation date is given\n",
        "\n",
        "--------------------------------------------------------------------\n",
        "INTERPRETATION RULES\n",
        "--------------------------------------------------------------------\n",
        "- Object number and secondary identifier must reflect your rules:\n",
        "    â€¢ object_number â†’ catalogue number BEFORE the name\n",
        "    â€¢ secondary_identifier â†’ object ID appearing later or at end of text\n",
        "- whole_or_part must be exactly \"Whole\" or \"Part\".\n",
        "- Item count must be numeric or \"N/A\".\n",
        "- Legal status:\n",
        "    â€¢ donor mentioned â†’ \"donated\"\n",
        "    â€¢ purchased â†’ \"purchased\"\n",
        "    â€¢ loaned â†’ \"loaned\"\n",
        "    â€¢ bequeathed â†’ \"bequeathed\"\n",
        "    â€¢ absent â†’ null\n",
        "- Maker roles must be explicit and accurate (maker, donor, inventor, user, etc.)\n",
        "- Place-related information must include the role (place made, place used, etc.)\n",
        "- Materials and hazards must only include items that the object is made out of or produces.\n",
        "- Measurements must be extracted verbatim.\n",
        "- Interpretation must include only interpretive content (purpose, significance, context).\n",
        "- Interpretation source includes only explicit references.\n",
        "- Any field not present â†’ return null.\n",
        "\n",
        "--------------------------------------------------------------------\n",
        "ADDITIONAL TASKS: LANGUAGE SENSITIVITY CHECKS\n",
        "--------------------------------------------------------------------\n",
        "Analyse the entry and identify:\n",
        "\n",
        "1. **HARMFUL LANGUAGE**\n",
        "   Includes (but is not limited to):\n",
        "   - derogatory descriptors\n",
        "   - racist/colonial terminology\n",
        "   - sexist terms\n",
        "   - ableist terms\n",
        "   - dehumanising phrasing\n",
        "   - offensive ethnonyms\n",
        "   - pejorative references to groups or individuals\n",
        "\n",
        "2. **OUTDATED / OBSOLETE LANGUAGE**\n",
        "   Includes but limited to:\n",
        "   - archaic ethnonyms\n",
        "   - obsolete medical or psychiatric terms\n",
        "   - outdated occupational labels\n",
        "   - antiquated or colonial-era descriptors\n",
        "   - archaic spellings or terminology no longer used today\n",
        "\n",
        "Return:\n",
        "- count of terms for each category\n",
        "- list of the exact words/phrases found\n",
        "\n",
        "--------------------------------------------------------------------\n",
        "OUTPUT FORMAT\n",
        "--------------------------------------------------------------------\n",
        "Return a single JSON object with the extraction schema above, AND two additional objects:\n",
        "\n",
        "\"harmful_language\": {{\n",
        "    \"count\": int,\n",
        "    \"terms\": [list]\n",
        "}},\n",
        "\"outdated_language\": {{\n",
        "    \"count\": int,\n",
        "    \"terms\": [list]\n",
        "}}\n",
        "\n",
        "JSON only. No commentary.\n",
        "\n",
        "--------------------------------------------------------------------\n",
        "INPUT\n",
        "--------------------------------------------------------------------\n",
        "{entry_text}\n",
        "\n",
        "--------------------------------------------------------------------\n",
        "OUTPUT\n",
        "--------------------------------------------------------------------\n",
        "JSON only.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Step 6: Resume support\n",
        "# ---------------------------------------------\n",
        "output_path = \"/content/drive/MyDrive/output.xlsx\"\n",
        "output_csv = \"/content/drive/MyDrive/output.csv\"\n",
        "log_path = \"/content/drive/MyDrive/output2.xlsx\"\n",
        "\n",
        "if os.path.exists(output_path):\n",
        "    out_df = pd.read_excel(output_path)\n",
        "    processed = set(out_df[\"RowIndex\"])\n",
        "    print(f\"Resuming from existing results ({len(processed)} processed).\")\n",
        "else:\n",
        "    out_df = pd.DataFrame()\n",
        "    processed = set()\n",
        "\n",
        "logs = []\n",
        "rows_out = []\n",
        "start_time_all = time.time()\n",
        "\n",
        "FAST_MODE = False\n",
        "MAX_ROWS = 10\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Step 7: Main Loop\n",
        "# ---------------------------------------------\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "\n",
        "    if FAST_MODE and idx >= MAX_ROWS:\n",
        "        break\n",
        "    if idx in processed:\n",
        "        continue\n",
        "\n",
        "    entry_text = \" \".join(\n",
        "        str(row[c]) for c in historic_cols if c in df.columns and pd.notna(row[c])\n",
        "    )\n",
        "    if not entry_text.strip():\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nðŸ”¹ Row {idx+1}/{len(df)} â€” extracting structured schema + language checks\")\n",
        "\n",
        "    prompt = make_prompt(entry_text)\n",
        "    result = call_gemini_with_retry(prompt)\n",
        "    if not result:\n",
        "        logs.append({\"RowIndex\": idx, \"Status\": \"Failed\"})\n",
        "        continue\n",
        "\n",
        "    # Store output\n",
        "    row_out = {\"RowIndex\": idx}\n",
        "    row_out.update(result)\n",
        "\n",
        "    rows_out.append(row_out)\n",
        "    out_df = pd.concat([out_df, pd.DataFrame([row_out])], ignore_index=True)\n",
        "\n",
        "    # Clean Excel characters\n",
        "    out_df = out_df.applymap(clean_excel_string)\n",
        "\n",
        "    # Save XLSX + CSV\n",
        "    out_df.to_excel(output_path, index=False)\n",
        "    out_df.to_csv(output_csv, index=False)\n",
        "\n",
        "    logs.append({\"RowIndex\": idx, \"Status\": \"Success\"})\n",
        "    pd.DataFrame(logs).to_excel(log_path, index=False)\n",
        "\n",
        "    print(\n",
        "        f\"Saved row {idx} â€” harmful={result['harmful_language']['count']} \"\n",
        "        f\"outdated={result['outdated_language']['count']}\"\n",
        "    )\n",
        "\n",
        "    time.sleep(random.uniform(1.0, 2.0))\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Final Summary\n",
        "# ---------------------------------------------\n",
        "print(\"Completed extraction.\")\n",
        "print(output_path)\n",
        "print(output_csv)\n",
        "print(log_path)\n",
        "\n",
        "files.download(output_path)\n",
        "files.download(output_csv)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}